<!DOCTYPE HTML>
<html lang="en">


<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Xizhi Xiao   |  肖茜之</title>
    <meta name="author" content="Yiran Geng">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
        <link rel="icon" type="image/png" href="images/icon/icon.jpg">
    <script>
    var _hmt = _hmt || [];
    (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?a4292ca8f2fe5fd7dc6dfd78cc894aab";
        var s = document.getElementsByTagName("script")[0]; 
        s.parentNode.insertBefore(hm, s);
    })();
    </script>
</head>


<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<tr style="padding:0px">
    <td style="padding:0px">
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr style="padding:0px">
        <td style="padding:2.5%;width:68%;vertical-align:middle">
            <p style="text-align:center">
            <name>Xizhi Xiao | 肖茜之</name>
            </p>
            <p>I am a sophomore at <a href="https://english.pku.edu.cn">Peking University</a>, majoring in Artificial Intelligence. I joined the <a href="https://tongclass.ac.cn/">Tong Class</a> (an AGI program founded by <a href="http://www.stat.ucla.edu/~sczhu/">Prof. Song-Chun Zhu</a>), being a member and the monitor in the class of 2021.
            </p>
            <p>
            I'm interested in cognitive reasoning and machine learning. In 2022 summer, I researched on social intelligence in nonverbal signals via physically grounded simulation under the guidance of <a href="https://www.psy.pku.edu.cn/english/people/faculty/professor/yujiapeng/index.htm">Prof. Yujia Peng</a> and <a href="https://lifengfan.github.io/">Dr. Lifeng Fan</a> at <a href="https://bigai.ai/">Beijing Institute for General Artificial Intelligence (BIGAI)</a>.
            </p>
            <p>
            In my spare time, I love dancing, baking and watching documentaries. These hobbies give me an opportunity to relax thoroughly, explore heartily and create freely.
            </p>
            <p style="text-align:center">
            <a href="mailto:xxz3470608107@stu.pku.edu.cn">Email</a> &nbsp/&nbsp
            <!-- <a href="data/XizhiXiao-CV.pdf">CV</a> &nbsp/&nbsp -->
            <!-- <a href="data/XizhiXiao-bio.txt">Bio</a> &nbsp/&nbsp -->
            <!-- <a href="https://twitter.com/XizhiXiao">Twitter</a> &nbsp/&nbsp -->
            <a href="https://github.com/Ladysausage/">Github</a>
            </p>
        </td>
        <td style="padding:2.5%;width:40%;max-width:40%">
            <a href="images/me.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
        </td>
        </tr>
    </tbody></table>
    <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Research</heading>
            <p>
            I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images. Representative papers are <span class="highlight">highlighted</span>.
            </p>
        </td>
        </tr>
    </tbody></table> -->
    

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <heading>Research</heading>
        <p>
            My research focuses on robotics, reinforcement learning, and computer vision. I also have a broad curiosity about computer science, mathematics, physics and art.
        </p>

        <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
                <div class="two" id='malle_image'>
                <img src='images/PartManip.png' width="194"></div>
                <img src='images/PartManip.png' width="194">
            </div>
            <script type="text/javascript">
                function malle_start() {
                document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
            </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://sites.google.com/view/pgvp/" target="_blank">
                <papertitle>PartManip: Learning Part-based Cross-Category Object Manipulation Policy from Point Cloud Observations</papertitle>
            </a>
            <br>
            <a:focus>Haoran Geng*</a:focus>,
            <a:focus>Ziming Li*</a:focus>,
            <a:focus><strong>Yiran Geng</strong></a:focus>,
            <a:focus>Jiayi Chen</a:focus>,
            <a:focus>Hao Dong</a:focus>,
            <a:focus>He Wang</a:focus>
            <br>
            <a>Paper (Coming Soon)</a>
            /
            <a href="https://pku-epic.github.io/PartManip/" target="_blank">Project Page</a>
            <br>
            <em>CVPR 2023</em>
            <p></p>
            
            <p>We introduce a large-scale, part-based, cross-category object manipulation benchmark with tasks in realistic, vision-based settings. </p>
            </td>
        </tr>

        <tr onmouseout="malle_stop()" onmouseover="malle_start()">
            <td style="padding:20px;width:30%;vertical-align:middle">
            <div class="one">
                <div class="two" id='malle_image'>
                <br>
                <img src='images/E2E_3.png' width="190"></div>
                <br>
                <img src='images/E2E_3.png' width="190">
            </div>
            <script type="text/javascript">
                function malle_start() {
                document.getElementById('malle_image').style.opacity = "1";
                }

                function malle_stop() {
                document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
            </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2209.12941">
                <papertitle><br>RLAfford: End-to-End Affordance Learning for Robotic Manipulation</papertitle>
            </a>
            <br>
            <a:focus><strong>Yiran Geng*</strong></a:focus>,
            <a:focus>Boshi An*</a:focus>,
            <a:focus>Haoran Geng</a:focus>,
            <a:focus href="https://cypypccpy.github.io/">Yuanpei Chen</a:focus>,
            <a:focus href="https://www.yangyaodong.com/">Yaodong Yang</a:focus>,
            <a:focus href="https://zsdonghao.github.io">Hao Dong</a:focus>
            <br>
            <a href="https://arxiv.org/abs/2209.12941">ArXiv</a>
            /
            <a href="https://sites.google.com/view/rlafford/">Project Page</a>
            /
            <a href="https://www.bilibili.com/video/BV1cM411b7ZD/?spm_id_from=444.41.list.card_archive.click">Video</a>
            /
            <a href="https://github.com/hyperplane-lab/RLAfford">Code</a>
            /
            <a href="https://github.com/GengYiran/Draw_PointCloud">Code (Renderer)</a>
            /
            <a href="https://github.com/boshi-an/SapienDataset">Dataset</a>
            /
            <a href="https://mp.weixin.qq.com/s?__biz=MzU0MjU5NjQ3NA==&mid=2247498641&idx=1&sn=06a3f2a314e43def32df5ee76e47fa55&chksm=fb1af784cc6d7e92852a53e994d57139d7beed0e379bf3ea7c5d66e4fe558b4a70d14f19832d&mpshare=1&scene=1&srcid=0125YlFVj1P4UMAt6V9CcKKq&sharer_sharetime=1674658986915&sharer_shareid=71465da75fb2e1df3c97cf572a6ce074#rd"
            >Media (CFCS)</a>
            /
            <a href="https://mp.weixin.qq.com/s?__biz=MzA3OTE0MjQzMw==&mid=2651954123&idx=1&sn=d4f6814fc92e6e7ded87f74ffedf723c&chksm=845d3085b32ab9937babd49f0a048340a00e847666dc7126ef1b1b04b48974cae90e32134db1&mpshare=1&scene=1&srcid=0214KpmupUsYZkxuL6TboxV1&sharer_sharetime=1676344711865&sharer_shareid=8d02b18bc72b08d54fb4e28c3b683968#rd
            ">Media (PKU)</a></li>
            <br>
            <em>ICRA 2023</em>
            <p></p>
            <p>In this study, we take advantage of visual affordance by using the contact information generated during the RL training process to predict contact maps of interest. </p>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
            <div class="one">
                <div class="two" id='malle_image'>
                <img src='images/GraspNerf.png' width="190"></div>
                <img src='images/GraspNerf.png' width="190">
            </div>
            <script type="text/javascript">
                function malle_start() {
                document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
            </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2210.06575">
                <papertitle><br>GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent and Specular Objects Using Generalizable NeRF</papertitle>
            </a>
            <br>
            <a:focus href="https://daiqy.github.io/">Qiyu Dai*</a:focus>,
            <a:focus>Yan Zhu*</a:focus>, 
            <a:focus><strong>Yiran Geng</strong></a:focus>,
            <a:focus>Ciyu Ruan</a:focus>,
            <a:focus href="https://www.researchgate.net/profile/Jiazhao-Zhang-2">Jiazhao Zhang</a:focus>,
            <a:focus href="https://hughw19.github.io/">He Wang</a:focus>
            <br>
            <a href="https://arxiv.org/abs/2210.06575">ArXiv</a>
            /
            <a href="https://pku-epic.github.io/GraspNeRF">Project Page</a>
            /
            <a href="https://github.com/PKU-EPIC/GraspNeRF">Code</a>
            /
            <a>Dataset (Coming Soon)</a>
            <br>
                <em>ICRA 2023</em>
            <p></p>
            <p>
                We propose a multiview RGB-based 6-DoF grasp detection network, GraspNeRF, that leverages the generalizable neural radiance field (NeRF) to achieve material-agnostic object grasping in clutter.
            </p>
            </td>
        </tr> -->


        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <heading>Preprints</heading>
            
        </tr>
        <td style="padding:20px;width:30%;vertical-align:middle">
            <div class="one">
            <div class="two" id='malle_image'>
                <img src='images/2022grasparl.gif' width="190"></div>
            <img src='images/2022grasparl.gif' width="190">
            </div>
            <script type="text/javascript">
            function malle_start() {
                document.getElementById('malle_image').style.opacity = "1";
            }

            function malle_stop() {
                document.getElementById('malle_image').style.opacity = "0";
            }
            malle_stop()
            </script>
        </td>
        <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2203.02119">
            <papertitle>GraspARL: Dynamic Grasping via Adversarial Reinforcement Learning</papertitle>
            </a>
            <br>
            <a:focus href="https://tianhaowuhz.github.io/">Tianhao Wu*</a:focus>, 
            <a:focus href="https://fangweizhong.xyz/">Fangwei Zhong*</a:focus>,
            <a:focus><strong>Yiran Geng</strong></a>,
            <a:focus href="https://www.yangyaodong.com/">Yaodong Yang</a:focus>,
            <a:focus href="https://cs.pku.edu.cn/info/1085/1331.htm">Yizhou Wang</a:focus>,
            <a:focus href="https://zsdonghao.github.io">Hao Dong</a:focus>
            <br>
            <a href="https://arxiv.org/abs/2203.02119">ArXiv</a>
            <br>
            <em>Under Review 2023</em>
            <p></p>
            <p>
            This study is the first attempt to formulate the dynamic grasping problem as a “move-and-grasp” game and use adversarial RL to train the grasping policy and object moving strategies jointly.
            </p>

        </td>    

        </tbody></table> -->


        <table width="100%" align="center" border="0" cellpadding="10"><tbody>
            <br>
                <heading>Experience</heading>
            <br>
            <!-- <tr>
                <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/experience/Bigai.png", width="100"></td>
                <td width="90%" valign="center">
                <strong>Beijing Institute for General Artificial Intelligence (BIGAI) </strong>
                <br> 2022.05 - Present
                <br> <strong>Research Intern</strong>
                <br> Research Advisor: Prof. <a href="https://www.yangyaodong.com/">Yaodong Yang</a> 
                <br> Academic Advisor: Prof. <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>
                </td>
            </tr> -->
            <tr>
                <br>
                <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/experience/PKU.png", width="100"></td>
                <td width="90%" valign="center">
                    <strong>Tong Class, Yuanpei College, Peking University (PKU)</strong>
                    <br> 2021.09 - Present
                    <br> <strong>Undergraduate Student</strong>
                    <!-- <br> Research Advisor: Prof. <a href="https://zsdonghao.github.io/">Hao Dong</a> -->
                    <br> Academic Advisor: Prof. <a href="https://yzhu.io">Yixin Zhu</a>
                </td>
            </tr>
            </tbody></table>


        <!-- <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <br>
            <heading>Services</heading>
            <br><br>
        <td style="padding:0px;width:100%;vertical-align:middle">
            <p>
            <li>Volunteer: WINE 2020</li>
            </p>
            <p>
            <li>Reviewer: CVPR2023, ICCV2023</li>
            </p>
        </td>
        </tr>
        </tbody></table> -->


        <!-- <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <br>
            <heading>Teaching Assistant</heading>
            <br><br>
        <td style="padding:0px;width:100%;vertical-align:middle">
            <p>
            <li>Probability Theory and Statistics (A) (Spring 2023)</li>
            </p>
            <p>
            <li>Study and Practice on Topics of Frontier Computing (II) (Prof. Hao Dong Track, Spring 2023) 
                [<a href="https://github.com/GengYiran/RL-Robot-Assignment#pre-requisite-knowledge">Assignment</a>]
            </li>
            </p>
        </td>
        </tr>
        </tbody></table> -->


        <!-- <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <br>
            <heading>Recent Talks</heading>
            <br><br>
            <td style="padding:0px;width:100%;vertical-align:middle">
            <p>
                <li>I'll give a talk at <a href="https://sites.google.com/view/myochallenge#h.t3275626vjox" target="_blank">NeurIPS 2022 MyoChallenge workshop</a> at Dec 7 from 21h to 24h (UTC time).
                <br>
                <a href="https://neurips.cc/virtual/2022/competition/50098">Link</a>
                /
                <a href="pdf/DieRotation_NIPS22.pdf">Slides</a></li>
            </p>
            <P>
                <li>I'll give a talk "Agent Manipulation Skill Learning through Reinforcement Learning" at <a href="https://cfcs.pku.edu.cn/announcement/events/cspeertalks/index.htm" target="_blank">CS Peer Talks</a> at CFCS, Peking University (Time TBD).
                <br>
                <a href="https://cfcs.pku.edu.cn/announcement/events/cspeertalks/index.htm">Link</a>
                </li>
            </P>
            <P>
                <li>I'll give a talk "End-to-End Affordance Learning for Robotic Manipulation" at <a href="https://mp.weixin.qq.com/s?__biz=MzU0MjU5NjQ3NA==&mid=2247498147&idx=1&sn=9039ff0cf8ea6f0a6654b4ea44b586bf&chksm=fb1af5b6cc6d7ca0328ab3a7dd20dff998d3eba7d472fd1b403e8ac6dc34fdffed20c0e37fe0&mpshare=1&scene=1&srcid=1129ZzNYSx7cW5qkvJYwLwdY&sharer_sharetime=1669702563247&sharer_shareid=71465da75fb2e1df3c97cf572a6ce074#rd" target="_blank">CFCS Frontier Storm</a> at CFCS, Peking University (Time TBD). 
                <br>
                <a href="https://mp.weixin.qq.com/s?__biz=MzU0MjU5NjQ3NA==&mid=2247498147&idx=1&sn=9039ff0cf8ea6f0a6654b4ea44b586bf&chksm=fb1af5b6cc6d7ca0328ab3a7dd20dff998d3eba7d472fd1b403e8ac6dc34fdffed20c0e37fe0&mpshare=1&scene=1&srcid=1129ZzNYSx7cW5qkvJYwLwdY&sharer_sharetime=1669702563247&sharer_shareid=71465da75fb2e1df3c97cf572a6ce074#rd">Link</a>
                /
                <a href="pdf/poster.pdf">Poster</a>
                </li>
            </P>
            </td>
        </tr>
        </tbody></table> -->


        <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                
                <br>
                <heading>Selected Awards and Honors</heading>
                <!-- <br> -->
                <br>
                <br>
            <td style="padding:0px;width:100%;vertical-align:middle">
                <p>
                <li>2021: First Class Scholarship for Freshmen of Peking University</li>
                </p>
                <p>
                <li>2021: Rank 1st/100k in National College Entrance Examination in Hunan Province</li>
                </p>
            </td>
            </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                    This template is a modification to <a href="https://jonbarron.info/">Jon Barron's website</a>. 
                </p>
            </td>
            </tr>
        </tbody></table>
    </td>
    </tr>
</table>
</body>
</html>
